{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning a generative AI Model for Dialgue Summarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "   tokenizers==0.12.1 \\\n",
    "   torch==1.13.1+cu117 torchvision>=0.13.1+cu117 torchaudio>=0.13.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117 --no-cache-dir \\\n",
    "   torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/david/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7212d0eb5947fea1dc7a33d60a1bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugginface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(hugginface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856 \n",
      "all model parameters: 247577856\n",
      "percentageof trainable model parameter: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params} \\nall model parameters: {all_model_params}\\npercentageof trainable model parameter: {((trainable_model_params/all_model_params)*100)}%\"\n",
    "\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summaise the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summaise the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "        original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\david\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-c8fac5d84cd35861\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-6a3be39e9e86e63d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49eae456da88406696a5bb493d0626f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\david\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-c8fac5d84cd35861\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b051e2995756c776.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarise the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    " \n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenizer_function code is handling all data accross all split in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\david\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-c8fac5d84cd35861\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ce129ebb82cb5839.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea90db9405b847ba8bd24af54fa6ba4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\david\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-c8fac5d84cd35861\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b6daf72a11c29c75.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the dataset:\n",
      "Training:(125, 2)\n",
      "Validation:(5, 2)\n",
      "Test:(15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the dataset:\")\n",
    "print(f\"Training:{tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation:{tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test:{tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output directory for fine tunned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./flan-dialogue-summary-checkpoint\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options to training on CUDA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.zeros.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=4,  # Set the batch size according to your GPU memory\n",
    "    per_device_eval_batch_size=4,  # Set the batch size according to your GPU memory\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients for larger effective batch size\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,  # Save checkpoint every 100 steps\n",
    "    report_to=\"none\",  # Disable logging\n",
    "    disable_tqdm=True,  # Disable tqdm progress bar\n",
    "    fp16=False,  # Enable mixed-precision training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 49.5312, 'learning_rate': 9.9e-06, 'epoch': 0.25}\n",
      "{'loss': 50.375, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.5}\n",
      "{'loss': 49.5625, 'learning_rate': 9.7e-06, 'epoch': 0.75}\n",
      "{'loss': 48.7188, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.0}\n",
      "{'loss': 50.0, 'learning_rate': 9.5e-06, 'epoch': 1.25}\n",
      "{'loss': 49.0625, 'learning_rate': 9.4e-06, 'epoch': 1.5}\n",
      "{'loss': 48.9688, 'learning_rate': 9.3e-06, 'epoch': 1.75}\n",
      "{'loss': 48.8438, 'learning_rate': 9.200000000000002e-06, 'epoch': 2.0}\n",
      "{'loss': 49.375, 'learning_rate': 9.100000000000001e-06, 'epoch': 2.25}\n",
      "{'loss': 48.25, 'learning_rate': 9e-06, 'epoch': 2.5}\n",
      "{'loss': 48.5938, 'learning_rate': 8.900000000000001e-06, 'epoch': 2.75}\n",
      "{'loss': 49.1875, 'learning_rate': 8.8e-06, 'epoch': 3.0}\n",
      "{'loss': 49.2188, 'learning_rate': 8.700000000000001e-06, 'epoch': 3.25}\n",
      "{'loss': 48.2188, 'learning_rate': 8.6e-06, 'epoch': 3.5}\n",
      "{'loss': 48.1875, 'learning_rate': 8.5e-06, 'epoch': 3.75}\n",
      "{'loss': 48.8125, 'learning_rate': 8.400000000000001e-06, 'epoch': 4.0}\n",
      "{'loss': 47.8125, 'learning_rate': 8.3e-06, 'epoch': 4.25}\n",
      "{'loss': 48.125, 'learning_rate': 8.2e-06, 'epoch': 4.5}\n",
      "{'loss': 47.8125, 'learning_rate': 8.1e-06, 'epoch': 4.75}\n",
      "{'loss': 47.9375, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.0}\n",
      "{'loss': 47.9062, 'learning_rate': 7.9e-06, 'epoch': 5.25}\n",
      "{'loss': 48.1875, 'learning_rate': 7.800000000000002e-06, 'epoch': 5.5}\n",
      "{'loss': 46.8438, 'learning_rate': 7.7e-06, 'epoch': 5.75}\n",
      "{'loss': 48.3125, 'learning_rate': 7.600000000000001e-06, 'epoch': 6.0}\n",
      "{'loss': 47.4062, 'learning_rate': 7.500000000000001e-06, 'epoch': 6.25}\n",
      "{'loss': 48.1562, 'learning_rate': 7.4e-06, 'epoch': 6.5}\n",
      "{'loss': 49.0938, 'learning_rate': 7.3e-06, 'epoch': 6.75}\n",
      "{'loss': 48.1562, 'learning_rate': 7.2000000000000005e-06, 'epoch': 7.0}\n",
      "{'loss': 46.3438, 'learning_rate': 7.100000000000001e-06, 'epoch': 7.25}\n",
      "{'loss': 47.9375, 'learning_rate': 7e-06, 'epoch': 7.5}\n",
      "{'loss': 48.3125, 'learning_rate': 6.9e-06, 'epoch': 7.75}\n",
      "{'loss': 48.1875, 'learning_rate': 6.800000000000001e-06, 'epoch': 8.0}\n",
      "{'loss': 48.25, 'learning_rate': 6.700000000000001e-06, 'epoch': 8.25}\n",
      "{'loss': 46.5, 'learning_rate': 6.600000000000001e-06, 'epoch': 8.5}\n",
      "{'loss': 48.375, 'learning_rate': 6.5000000000000004e-06, 'epoch': 8.75}\n",
      "{'loss': 48.0938, 'learning_rate': 6.4000000000000006e-06, 'epoch': 9.0}\n",
      "{'loss': 47.8438, 'learning_rate': 6.300000000000001e-06, 'epoch': 9.25}\n",
      "{'loss': 46.4062, 'learning_rate': 6.200000000000001e-06, 'epoch': 9.5}\n",
      "{'loss': 47.6875, 'learning_rate': 6.1e-06, 'epoch': 9.75}\n",
      "{'loss': 47.875, 'learning_rate': 6e-06, 'epoch': 10.0}\n",
      "{'loss': 46.9375, 'learning_rate': 5.9e-06, 'epoch': 10.25}\n",
      "{'loss': 48.0938, 'learning_rate': 5.8e-06, 'epoch': 10.5}\n",
      "{'loss': 47.75, 'learning_rate': 5.7e-06, 'epoch': 10.75}\n",
      "{'loss': 47.2188, 'learning_rate': 5.600000000000001e-06, 'epoch': 11.0}\n",
      "{'loss': 47.8125, 'learning_rate': 5.500000000000001e-06, 'epoch': 11.25}\n",
      "{'loss': 47.5625, 'learning_rate': 5.400000000000001e-06, 'epoch': 11.5}\n",
      "{'loss': 47.0312, 'learning_rate': 5.300000000000001e-06, 'epoch': 11.75}\n",
      "{'loss': 46.2188, 'learning_rate': 5.2e-06, 'epoch': 12.0}\n",
      "{'loss': 47.7812, 'learning_rate': 5.1e-06, 'epoch': 12.25}\n",
      "{'loss': 46.2188, 'learning_rate': 5e-06, 'epoch': 12.5}\n",
      "{'loss': 47.4688, 'learning_rate': 4.9000000000000005e-06, 'epoch': 12.75}\n",
      "{'loss': 46.9375, 'learning_rate': 4.800000000000001e-06, 'epoch': 13.0}\n",
      "{'loss': 46.7812, 'learning_rate': 4.7e-06, 'epoch': 13.25}\n",
      "{'loss': 47.1562, 'learning_rate': 4.600000000000001e-06, 'epoch': 13.5}\n",
      "{'loss': 47.6875, 'learning_rate': 4.5e-06, 'epoch': 13.75}\n",
      "{'loss': 46.1875, 'learning_rate': 4.4e-06, 'epoch': 14.0}\n",
      "{'loss': 47.375, 'learning_rate': 4.3e-06, 'epoch': 14.25}\n",
      "{'loss': 47.1562, 'learning_rate': 4.2000000000000004e-06, 'epoch': 14.5}\n",
      "{'loss': 47.1875, 'learning_rate': 4.1e-06, 'epoch': 14.75}\n",
      "{'loss': 47.625, 'learning_rate': 4.000000000000001e-06, 'epoch': 15.0}\n",
      "{'loss': 47.0625, 'learning_rate': 3.900000000000001e-06, 'epoch': 15.25}\n",
      "{'loss': 47.3125, 'learning_rate': 3.8000000000000005e-06, 'epoch': 15.5}\n",
      "{'loss': 46.8125, 'learning_rate': 3.7e-06, 'epoch': 15.75}\n",
      "{'loss': 46.6562, 'learning_rate': 3.6000000000000003e-06, 'epoch': 16.0}\n",
      "{'loss': 47.0938, 'learning_rate': 3.5e-06, 'epoch': 16.25}\n",
      "{'loss': 47.3125, 'learning_rate': 3.4000000000000005e-06, 'epoch': 16.5}\n",
      "{'loss': 47.25, 'learning_rate': 3.3000000000000006e-06, 'epoch': 16.75}\n",
      "{'loss': 47.0312, 'learning_rate': 3.2000000000000003e-06, 'epoch': 17.0}\n",
      "{'loss': 47.5, 'learning_rate': 3.1000000000000004e-06, 'epoch': 17.25}\n",
      "{'loss': 46.5312, 'learning_rate': 3e-06, 'epoch': 17.5}\n",
      "{'loss': 46.5938, 'learning_rate': 2.9e-06, 'epoch': 17.75}\n",
      "{'loss': 46.6875, 'learning_rate': 2.8000000000000003e-06, 'epoch': 18.0}\n",
      "{'loss': 46.0625, 'learning_rate': 2.7000000000000004e-06, 'epoch': 18.25}\n",
      "{'loss': 47.625, 'learning_rate': 2.6e-06, 'epoch': 18.5}\n",
      "{'loss': 46.7188, 'learning_rate': 2.5e-06, 'epoch': 18.75}\n",
      "{'loss': 48.375, 'learning_rate': 2.4000000000000003e-06, 'epoch': 19.0}\n",
      "{'loss': 47.1875, 'learning_rate': 2.3000000000000004e-06, 'epoch': 19.25}\n",
      "{'loss': 46.0938, 'learning_rate': 2.2e-06, 'epoch': 19.5}\n",
      "{'loss': 47.3438, 'learning_rate': 2.1000000000000002e-06, 'epoch': 19.75}\n",
      "{'loss': 46.4062, 'learning_rate': 2.0000000000000003e-06, 'epoch': 20.0}\n",
      "{'loss': 47.125, 'learning_rate': 1.9000000000000002e-06, 'epoch': 20.25}\n",
      "{'loss': 46.9688, 'learning_rate': 1.8000000000000001e-06, 'epoch': 20.5}\n",
      "{'loss': 46.8438, 'learning_rate': 1.7000000000000002e-06, 'epoch': 20.75}\n",
      "{'loss': 46.7812, 'learning_rate': 1.6000000000000001e-06, 'epoch': 21.0}\n",
      "{'loss': 47.1562, 'learning_rate': 1.5e-06, 'epoch': 21.25}\n",
      "{'loss': 47.3125, 'learning_rate': 1.4000000000000001e-06, 'epoch': 21.5}\n",
      "{'loss': 46.125, 'learning_rate': 1.3e-06, 'epoch': 21.75}\n",
      "{'loss': 48.0625, 'learning_rate': 1.2000000000000002e-06, 'epoch': 22.0}\n",
      "{'loss': 47.125, 'learning_rate': 1.1e-06, 'epoch': 22.25}\n",
      "{'loss': 46.2188, 'learning_rate': 1.0000000000000002e-06, 'epoch': 22.5}\n",
      "{'loss': 46.5938, 'learning_rate': 9.000000000000001e-07, 'epoch': 22.75}\n",
      "{'loss': 47.2812, 'learning_rate': 8.000000000000001e-07, 'epoch': 23.0}\n",
      "{'loss': 47.0625, 'learning_rate': 7.000000000000001e-07, 'epoch': 23.25}\n",
      "{'loss': 46.5625, 'learning_rate': 6.000000000000001e-07, 'epoch': 23.5}\n",
      "{'loss': 46.8125, 'learning_rate': 5.000000000000001e-07, 'epoch': 23.75}\n",
      "{'loss': 46.8125, 'learning_rate': 4.0000000000000003e-07, 'epoch': 24.0}\n",
      "{'loss': 46.6875, 'learning_rate': 3.0000000000000004e-07, 'epoch': 24.25}\n",
      "{'loss': 48.0938, 'learning_rate': 2.0000000000000002e-07, 'epoch': 24.5}\n",
      "{'loss': 46.5625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 24.75}\n",
      "{'loss': 46.75, 'learning_rate': 0.0, 'epoch': 25.0}\n",
      "{'eval_loss': 50.45000076293945, 'eval_runtime': 0.1924, 'eval_samples_per_second': 25.989, 'eval_steps_per_second': 10.396, 'epoch': 25.0}\n",
      "{'train_runtime': 332.3613, 'train_samples_per_second': 9.628, 'train_steps_per_second': 0.301, 'train_loss': 47.5525, 'epoch': 25.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=47.5525, metrics={'train_runtime': 332.3613, 'train_samples_per_second': 9.628, 'train_steps_per_second': 0.301, 'train_loss': 47.5525, 'epoch': 25.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#progress_bar = tqdm(total=training_args.max_steps, desc=\"Training\", dynamic_ncols=True, no_deprecation_warning=True )\n",
    "trainer.train(resume_from_checkpoint=None)\n",
    "#progress_bar.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Oringinal Model:\n",
      "What is your system's hardware requirements?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model:\n",
      "#Person1#: You need to upgrade your system and some hardware, and possibly some software that will allow you to run your own business. We've tried other parts but are not sure which one is best for you.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogues = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summaise the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Remove # if you have the ability to run both models\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, do_sample=True, tempature=0.1, num_beams=1))\n",
    "original_model_text_outputs = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, do_sample=True, tempature=0.1, num_beams=1))\n",
    "instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "\n",
    "# Remove # if you have the ability to run both models\n",
    "print(dash_line)\n",
    "print(f'Original Model:\\n{original_model_text_outputs}\\n')\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Instruct Model:\\n{instruct_model_text_outputs}')\n",
    "print(dash_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summary</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              human_baseline_summary  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  #Person1#: Happy birthday, Brian. #Person2#: I...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  #Person1#: Happy birthday, Brian. #Person2#: I...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "Summaise the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "original_model_text_outputs = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "original_model_summaries.append(original_model_text_outputs)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "instruct_model_summaries.append(instruct_model_text_outputs)\n",
    "\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns= ['human_baseline_summary', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL: \n",
      "{'rouge1': 0.05940594059405941, 'rouge2': 0.0, 'rougeL': 0.039603960396039604, 'rougeLsum': 0.039603960396039604}\n",
      "INSTRUCT MODEL: \n",
      "{'rouge1': 0.05940594059405941, 'rouge2': 0.0, 'rougeL': 0.039603960396039604, 'rougeLsum': 0.039603960396039604}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL: ')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL: ')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
